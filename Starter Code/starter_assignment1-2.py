# -*- coding: utf-8 -*-
"""starter_assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E47uJdpxFhVEHbcpihC_BWbRYMuXGCej

# Import necessary libraries
"""

import keras
from keras.models import Sequential
from keras.layers import Activation,Dense,Dropout,Conv2D,Flatten,MaxPooling2D
from keras.datasets import cifar100
from keras import optimizers
from matplotlib import pyplot as plt

import numpy as np
from keras import backend as K
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback, EarlyStopping
import tensorflow as tf
import datetime, os

"""# Load training data"""

batch_size = 128
num_classes = 100

# load your data and view the shapes (fill out below)
(X_train, y_train), (X_test, y_test) = ...

print("X_train original shape", X_train.shape)
print("y_train original shape", y_train.shape)

img_width, img_height, img_num_channels = X_train.shape[1:4]
input_shape = (img_width, img_height, img_num_channels)

# Visulize the first 5 elements in our data
for i in range(5):
    plt.subplot(3,3,i+1)
    plt.imshow(X_train[i], cmap='gray', interpolation='none')

"""# Format the data for training

- Converting to float32 and performing normalization ensures our data is in the correct format for our neural network.
"""

# 1. conversion of the dataset to float 32
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# 2. Normalization (fill out below)
...

print('x_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

"""One-Hot format: Represent categorical variables as binary vectors. It is used for the multi-class classification problems.

* "cat" -> [1, 0, 0]
* "dog" -> [0, 1, 0]
* "bird" -> [0, 0, 1]
"""

# Modify the label vectros to be in the one-hot format (fill out below)
y_train = keras.utils.to_categorical(...)
y_test = keras.utils.to_categorical(...)

print(y_train)
print(y_test)

"""# Build a sequential convolutional model"""

model = Sequential()

# first block
model.add(...)

# second block

# third block

# flatten to 1D vector

# output layer

!pip install git+https://github.com/paulgavrikov/visualkeras --upgrade

import visualkeras
visualkeras.layered_view(model, legend=True)

"""# Compile the model
- Choose your loss, optimizers, and any other metrics (e.g. Training accuracy, Test accuracy)
"""

model.compile(...)

"""# Train the model"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension, do not have to touch this.
# %load_ext tensorboard

"""Early Stopiing

* With more epochs, we can decrease our loss over time. This may lead to overfitting.
* Early stopping allows us to stop training when there is an increase in the loss value compared to the previous epoch.
* Detailed Arguments: https://keras.io/api/callbacks/early_stopping/
"""

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8, restore_best_weights = True)

# For the TensorBoard, set the directory you want to save your log.
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S")) # log directory will be created
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)


history = model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=100,
          verbose=1,
          callbacks=[early_stop,tensorboard_callback],
          validation_data=(X_test, y_test))

# save the model
model.save("best_conv.h5")
print("Model saved successfully")

"""Basics of Tensorboard, you can change options to change the shapes of scalars.

https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

"""# Evaluate its performance"""

score = model.evaluate(X_test, y_test, batch_size=50,
                                      steps=X_test.shape[0] // 50)

print('Test loss: %.2f' % score[0])
print('Test accuracy: %.2f'% score[1])

"""## Results

Use matplotlib to draw your results.
"""

import matplotlib.pyplot as plt

score = model.evaluate(X_test, y_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')

plt.figure(figsize=(10, 5))

# here we plot our validation curves as an example.

plt.subplot(1, 2, 1)
# fill in below
plt.plot(...)
plt.title('Validation loss history')
plt.ylabel('Loss value')
plt.xlabel('No. epoch')

plt.subplot(1, 2, 2)
# fill in below
plt.plot()
plt.title('Validation accuracy history')
plt.ylabel('Accuracy value (%)')
plt.xlabel('No. epoch')

plt.tight_layout()
plt.show()

"""Display images and labels here."""

# start here with:
predictions = model.predict(X_test)

...

# show the images and labels:

...

"""Here is sample code to help you think through getting the string labels, rather than the one hot encoded prediction output that looks like [0,0,0,0,1,...]."""

# define the class labels, may need to look them up
fine_labels = [...]

# test printing and indexing
print(class_labels[0])

# returning the label associated with the prediction's index
def convert_to_string(prediction, labels):
    # https://numpy.org/doc/stable/reference/generated/numpy.argmax.htmlax
    index = np.argmax(prediction)

    return labels[index]

prediction = [...]
label = convert_to_string(prediction, class_labels)

print(label)